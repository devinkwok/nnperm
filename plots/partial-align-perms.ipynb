{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layers.0.conv.weight', 'layers.0.conv.bias', 'layers.0.bn.layernorm.weight', 'layers.0.bn.layernorm.bias', 'layers.1.conv.weight', 'layers.1.conv.bias', 'layers.1.bn.layernorm.weight', 'layers.1.bn.layernorm.bias', 'layers.3.conv.weight', 'layers.3.conv.bias', 'layers.3.bn.layernorm.weight', 'layers.3.bn.layernorm.bias', 'layers.4.conv.weight', 'layers.4.conv.bias', 'layers.4.bn.layernorm.weight', 'layers.4.bn.layernorm.bias', 'layers.6.conv.weight', 'layers.6.conv.bias', 'layers.6.bn.layernorm.weight', 'layers.6.bn.layernorm.bias', 'layers.7.conv.weight', 'layers.7.conv.bias', 'layers.7.bn.layernorm.weight', 'layers.7.bn.layernorm.bias', 'layers.8.conv.weight', 'layers.8.conv.bias', 'layers.8.bn.layernorm.weight', 'layers.8.bn.layernorm.bias', 'layers.10.conv.weight', 'layers.10.conv.bias', 'layers.10.bn.layernorm.weight', 'layers.10.bn.layernorm.bias', 'layers.11.conv.weight', 'layers.11.conv.bias', 'layers.11.bn.layernorm.weight', 'layers.11.bn.layernorm.bias', 'layers.12.conv.weight', 'layers.12.conv.bias', 'layers.12.bn.layernorm.weight', 'layers.12.bn.layernorm.bias', 'layers.14.conv.weight', 'layers.14.conv.bias', 'layers.14.bn.layernorm.weight', 'layers.14.bn.layernorm.bias', 'layers.15.conv.weight', 'layers.15.conv.bias', 'layers.15.bn.layernorm.weight', 'layers.15.bn.layernorm.bias', 'layers.16.conv.weight', 'layers.16.conv.bias', 'layers.16.bn.layernorm.weight', 'layers.16.bn.layernorm.bias', 'fc.weight', 'fc.bias']\n",
      "cifar_vgg_16_16 lottery_2915b34d8b29a209ffee2288466cf5f6_1_2_pretrain_ep160_linear\n",
      "Pseq_0-in 16 0.0 0.6875 0.3125\n",
      "Pseq_4-layers.1.conv.weight 16 0.0 0.5625 0.4375\n",
      "Pseq_8-layers.3.conv.weight 32 0.0 0.59375 0.40625\n",
      "Pseq_12-layers.4.conv.weight 32 0.0 0.65625 0.34375\n",
      "Pseq_16-layers.6.conv.weight 64 0.046875 0.578125 0.375\n",
      "Pseq_20-layers.7.conv.weight 64 0.0 0.625 0.375\n",
      "Pseq_24-layers.8.conv.weight 64 0.125 0.625 0.25\n",
      "Pseq_28-layers.10.conv.weight 128 0.0078125 0.71875 0.2734375\n",
      "Pseq_32-layers.11.conv.weight 128 0.0 0.7578125 0.2421875\n",
      "Pseq_36-layers.12.conv.weight 128 0.0078125 0.7734375 0.21875\n",
      "Pseq_40-layers.14.conv.weight 128 0.015625 0.8359375 0.1484375\n",
      "Pseq_44-layers.15.conv.weight 128 0.0078125 0.796875 0.1953125\n",
      "Pseq_48-layers.16.conv.weight 128 0.0 0.9609375 0.0390625\n",
      "Pseq_0-in 16 0.0 0.8125 0.1875\n",
      "Pseq_4-layers.1.conv.weight 16 0.0 0.75 0.25\n",
      "Pseq_8-layers.3.conv.weight 32 0.0 0.6875 0.3125\n",
      "Pseq_12-layers.4.conv.weight 32 0.0 0.625 0.375\n",
      "Pseq_16-layers.6.conv.weight 64 0.03125 0.75 0.21875\n",
      "Pseq_20-layers.7.conv.weight 64 0.0 0.703125 0.296875\n",
      "Pseq_24-layers.8.conv.weight 64 0.015625 0.671875 0.3125\n",
      "Pseq_28-layers.10.conv.weight 128 0.0 0.71875 0.28125\n",
      "Pseq_32-layers.11.conv.weight 128 0.015625 0.7421875 0.2421875\n",
      "Pseq_36-layers.12.conv.weight 128 0.0078125 0.71875 0.2734375\n",
      "Pseq_40-layers.14.conv.weight 128 0.0 0.7109375 0.2890625\n",
      "Pseq_44-layers.15.conv.weight 128 0.0078125 0.734375 0.2578125\n",
      "Pseq_48-layers.16.conv.weight 128 0.0078125 0.734375 0.2578125\n",
      "cifar_vgg_16_8 lottery_3d9c91d3d4133cfcdcb2006da1507cbb_1_2_pretrain_ep160_linear\n",
      "Pseq_0-in 8 0.0 0.375 0.625\n",
      "Pseq_4-layers.1.conv.weight 8 0.0 0.625 0.375\n",
      "Pseq_8-layers.3.conv.weight 16 0.0 0.4375 0.5625\n",
      "Pseq_12-layers.4.conv.weight 16 0.0 0.4375 0.5625\n",
      "Pseq_16-layers.6.conv.weight 32 0.0 0.375 0.625\n",
      "Pseq_20-layers.7.conv.weight 32 0.03125 0.28125 0.6875\n",
      "Pseq_24-layers.8.conv.weight 32 0.25 0.34375 0.40625\n",
      "Pseq_28-layers.10.conv.weight 64 0.0 0.5 0.5\n",
      "Pseq_32-layers.11.conv.weight 64 0.0 0.625 0.375\n",
      "Pseq_36-layers.12.conv.weight 64 0.0 0.59375 0.40625\n",
      "Pseq_40-layers.14.conv.weight 64 0.015625 0.734375 0.25\n",
      "Pseq_44-layers.15.conv.weight 64 0.0 0.640625 0.359375\n",
      "Pseq_48-layers.16.conv.weight 64 0.015625 0.796875 0.1875\n",
      "Pseq_0-in 8 0.0 0.625 0.375\n",
      "Pseq_4-layers.1.conv.weight 8 0.125 0.625 0.25\n",
      "Pseq_8-layers.3.conv.weight 16 0.0 0.8125 0.1875\n",
      "Pseq_12-layers.4.conv.weight 16 0.0625 0.75 0.1875\n",
      "Pseq_16-layers.6.conv.weight 32 0.0 0.84375 0.15625\n",
      "Pseq_20-layers.7.conv.weight 32 0.03125 0.78125 0.1875\n",
      "Pseq_24-layers.8.conv.weight 32 0.03125 0.75 0.21875\n",
      "Pseq_28-layers.10.conv.weight 64 0.0 0.75 0.25\n",
      "Pseq_32-layers.11.conv.weight 64 0.03125 0.78125 0.1875\n",
      "Pseq_36-layers.12.conv.weight 64 0.015625 0.828125 0.15625\n",
      "Pseq_40-layers.14.conv.weight 64 0.0 0.78125 0.21875\n",
      "Pseq_44-layers.15.conv.weight 64 0.015625 0.796875 0.1875\n",
      "Pseq_48-layers.16.conv.weight 64 0.03125 0.703125 0.265625\n",
      "cifar_vgg_16_32 lottery_c855d7c25ffef997a89799dc08931e82_1_2_pretrain_ep160_linear\n",
      "Pseq_0-in 32 0.0 1.0 0.0\n",
      "Pseq_4-layers.1.conv.weight 32 0.0625 0.9375 0.0\n",
      "Pseq_8-layers.3.conv.weight 64 0.015625 0.984375 0.0\n",
      "Pseq_12-layers.4.conv.weight 64 0.0 1.0 0.0\n",
      "Pseq_16-layers.6.conv.weight 128 0.0078125 0.9921875 0.0\n",
      "Pseq_20-layers.7.conv.weight 128 0.0 1.0 0.0\n",
      "Pseq_24-layers.8.conv.weight 128 0.0078125 0.9921875 0.0\n",
      "Pseq_28-layers.10.conv.weight 256 0.00390625 0.99609375 0.0\n",
      "Pseq_32-layers.11.conv.weight 256 0.0078125 0.9921875 0.0\n",
      "Pseq_36-layers.12.conv.weight 256 0.0078125 0.9921875 0.0\n",
      "Pseq_40-layers.14.conv.weight 256 0.00390625 0.99609375 0.0\n",
      "Pseq_44-layers.15.conv.weight 256 0.00390625 0.99609375 0.0\n",
      "Pseq_48-layers.16.conv.weight 256 0.00390625 0.99609375 0.0\n",
      "Pseq_0-in 32 0.0 1.0 0.0\n",
      "Pseq_4-layers.1.conv.weight 32 0.0 1.0 0.0\n",
      "Pseq_8-layers.3.conv.weight 64 0.015625 0.984375 0.0\n",
      "Pseq_12-layers.4.conv.weight 64 0.0625 0.9375 0.0\n",
      "Pseq_16-layers.6.conv.weight 128 0.015625 0.984375 0.0\n",
      "Pseq_20-layers.7.conv.weight 128 0.0 1.0 0.0\n",
      "Pseq_24-layers.8.conv.weight 128 0.0 1.0 0.0\n",
      "Pseq_28-layers.10.conv.weight 256 0.0078125 0.9921875 0.0\n",
      "Pseq_32-layers.11.conv.weight 256 0.0 1.0 0.0\n",
      "Pseq_36-layers.12.conv.weight 256 0.00390625 0.99609375 0.0\n",
      "Pseq_40-layers.14.conv.weight 256 0.00390625 0.99609375 0.0\n",
      "Pseq_44-layers.15.conv.weight 256 0.0 1.0 0.0\n",
      "Pseq_48-layers.16.conv.weight 256 0.01171875 0.98828125 0.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nnperm.perm import Permutations, PermutationSpec, perm_compose, perm_inverse\n",
    "from nnperm.utils import get_open_lth_model\n",
    "\n",
    "model_name_to_dir = {\n",
    "    \"cifar_vgg_16_8\": \"lottery_3d9c91d3d4133cfcdcb2006da1507cbb\",\n",
    "    \"cifar_vgg_16_16\": \"lottery_2915b34d8b29a209ffee2288466cf5f6\",\n",
    "    \"cifar_vgg_16_32\": \"lottery_c855d7c25ffef997a89799dc08931e82\",\n",
    "    \"cifar_vgg_16_128\": \"lottery_8d561a7b273e4d6b2705ba6d627a69bd\",\n",
    "    \"cifar_vgg_16_256\": \"lottery_a309ac4ab15380928661e70ca8b054a1\",\n",
    "}\n",
    "# target_sizes = get_model_perm_size(\"lottery_2915b34d8b29a209ffee2288466cf5f6\")\n",
    "model = get_open_lth_model(Path(f\"../refactor-outputs/ckpts/lottery_3d9c91d3d4133cfcdcb2006da1507cbb/dummy_ckpt.pth\"), \"cpu\")\n",
    "print(list(model.state_dict().keys()))\n",
    "\n",
    "def get_model_perm_size(model_name):\n",
    "    model = get_open_lth_model(Path(f\"../refactor-outputs/ckpts/{model_name}/dummy_ckpt.pth\"), \"cpu\")\n",
    "    perm_spec = PermutationSpec.from_sequential_model(model.state_dict())\n",
    "    return perm_spec.get_sizes(model.state_dict())\n",
    "\n",
    "def classify_perm_fractions(perms_a, perms_b, align_sizes):\n",
    "    for k in perms_a.keys():\n",
    "        max_size = max(len(perms_a[k]), len(perms_b[k]))\n",
    "        n_align = align_sizes[k]\n",
    "        perm_a = perms_a[k][:n_align]\n",
    "        perm_b = perms_b[k][:n_align]\n",
    "        set_a = np.zeros(max_size, dtype=bool)\n",
    "        set_a[perm_a] = True\n",
    "        set_b = np.zeros(max_size, dtype=bool)\n",
    "        set_b[perm_b] = True\n",
    "        identical = (perm_a == perm_b)\n",
    "        repermuted = set_a * set_b\n",
    "        repermuted_a = np.logical_and(repermuted[perm_a], perm_a != perm_b)\n",
    "        repermuted_b = np.logical_and(repermuted[perm_b], perm_a != perm_b)\n",
    "        assert np.all(np.sum(repermuted_a) == np.sum(repermuted_b))\n",
    "        unaligned = np.logical_xor(set_a, set_b)\n",
    "        unaligned_a = unaligned[perm_a]\n",
    "        unaligned_b = unaligned[perm_b]\n",
    "        assert np.all(np.sum(unaligned_a) == np.sum(unaligned_b))\n",
    "        assert np.all(identical + repermuted_a + unaligned_a == 1)\n",
    "        assert np.all(identical + repermuted_b + unaligned_b == 1)\n",
    "        yield k, n_align, identical, repermuted_a, repermuted_b, unaligned_a, unaligned_b\n",
    "\n",
    "stats_dir = Path(\"../refactor-outputs/fix-embed-lottery_c855d7c25ffef997a89799dc08931e82/\")\n",
    "for file in stats_dir.glob(\"*linear.pt\"):\n",
    "    name = file.stem\n",
    "    for k, v in model_name_to_dir.items():\n",
    "        if v in file.stem:\n",
    "            name = k\n",
    "            key = v\n",
    "    direct_perm_stats_dict = torch.load(f\"../refactor-outputs/kernel-test/scratch/open_lth_data/{key}_1_2_pretrain_ep160_linear.pt\")\n",
    "    print(name, file.stem)\n",
    "    sizes = get_model_perm_size(key)\n",
    "    stats_dict = torch.load(file)\n",
    "    perm_a = Permutations(stats_dict[\"perm_a\"])\n",
    "    perm_b = Permutations(stats_dict[\"perm_b\"])\n",
    "    direct_perm = Permutations(direct_perm_stats_dict['perm'])\n",
    "    target_sizes = stats_dict[\"target_sizes\"]\n",
    "    for k, n, identical, r_a, r_b, u_a, u_b in classify_perm_fractions(perm_a, perm_b, sizes):\n",
    "        print(k, n, np.sum(identical) / n, np.sum(r_a) / n, np.sum(u_a) / n)\n",
    "    for k, n, identical, r_a, r_b, u_a, u_b in classify_perm_fractions(direct_perm, perm_a.inverse().compose(perm_b), sizes):\n",
    "        print(k, n, np.sum(identical) / n, np.sum(r_a) / n, np.sum(u_a) / n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35841c5c5f5b98e877ffc4e087c60be1ff37bef020851866405424a2c631e60f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
